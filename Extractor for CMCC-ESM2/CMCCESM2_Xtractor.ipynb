{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4372dc6-1c8b-482f-b293-e6174bb6e464",
   "metadata": {},
   "source": [
    "# Data Extractor for CMCC-ESM2 NetCDF files\n",
    "## The following snippets are for Amon, Omon datasets with a specific name format\n",
    "\n",
    "This notebook is specifically written/hardcoded for CMCC-ESM2 .nc files (Lovato et al., 2021) sourced from ESGF. The purpose was to extract timeseries data from specific nc files (i.e pr, thetao, so etc) to be used as datasets for applying the methodology I developed for selecting representative years. \n",
    "\n",
    "Note: These codes/snippets are hardcoded for nc files which are of Omon, Amon resolution. For Omon, it is specifically coded for variables which have 4 dimensions e.g thetao with [t,lev,i,j] dimensions or pr with [t, i, j] dimensions. The code has been tested for pr, thetao, so, chl parameters of Amon and Omon resolutions only. Please check the dimension of your netcdf file and modify the snippets accordingly in your device if you wish to use the same approach for extracting data from nc files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4dcdab-c26f-413a-b414-316e0016cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_Amon_CMCC-ESM2_ssp126_r1i1p1f1_gn_201501-210012.nc\n",
      "pr\n",
      "126\n",
      "amon\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your NetCDF files\n",
    "filepaths = sorted(glob('H:/M.Sc Thesis - Data, Methodology, Results/NetCDF Files/CMCC-ESM2 NC/126/pr_Amon_CMCC-ESM2_ssp126_r1i1p1f1_gn_*.nc'))\n",
    "filename = os.path.basename(filepaths[0])\n",
    "parts = filename.split('_')\n",
    "var = parts[0]\n",
    "freq = parts[1]\n",
    "scenario = parts[3]\n",
    "ssp = scenario[-3:]\n",
    "data_type = freq.lower()\n",
    "\n",
    "print(filename)\n",
    "print(var)\n",
    "print(ssp)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c330549-77d2-4fa2-b137-a4f20d1a0f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file FINO2_pr_126_ts.csv has been written.\n",
      "The file FINO3_pr_126_ts.csv has been written.\n",
      "The file Borssele_pr_126_ts.csv has been written.\n",
      "The file Belwind_pr_126_ts.csv has been written.\n",
      "The file Anholt_pr_126_ts.csv has been written.\n",
      "The file Samso_pr_126_ts.csv has been written.\n"
     ]
    }
   ],
   "source": [
    "location_type = 'point' # 'domain'  # Input the type of data you need to export\n",
    "\n",
    "''' \n",
    "The following snippet is for point and domain coordinates of locations. This snippet works ONLY for Amon and Omon datasets of CMCC-EMS2\n",
    "'''\n",
    "\n",
    "# Load data\n",
    "if location_type == 'point':\n",
    "    locations_data = pd.read_csv('pointxy.csv')   #sample coordinate file\n",
    "    location_names = locations_data['Location Name']\n",
    "    location_lons = locations_data['Longitude']\n",
    "    location_lats = locations_data['Latitude']\n",
    "    \n",
    "elif location_type == 'domain':\n",
    "    boundary_data = pd.read_csv('domainxy.csv')  #sample coordinate file\n",
    "    boundary_lons = boundary_data['Longitude']\n",
    "    boundary_lats = boundary_data['Latitude']\n",
    "    lat_min, lat_max = min(boundary_lats), max(boundary_lats)\n",
    "    lon_min, lon_max = min(boundary_lons), max(boundary_lons)\n",
    "\n",
    "\n",
    "# Initialize data storage\n",
    "time_series_data = {}\n",
    "\n",
    "if location_type == 'point':\n",
    "    for name in location_names:\n",
    "        time_series_data[name] = []\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        dataset = nc.Dataset(filepath, mode='r')\n",
    "        if data_type == 'omon':\n",
    "            lats = dataset.variables['latitude'][:]\n",
    "            lons = dataset.variables['longitude'][:]\n",
    "        elif data_type == 'amon':\n",
    "            lats = dataset.variables['lat'][:]\n",
    "            lons = dataset.variables['lon'][:]\n",
    "        time = dataset.variables['time'][:]\n",
    "        time_units = dataset.variables['time'].units\n",
    "        time_calendar = dataset.variables['time'].calendar\n",
    "        times = nc.num2date(time, units=time_units, calendar=time_calendar)\n",
    "        \n",
    "\n",
    "        # Identify node indices for the given locations\n",
    "        node_indices = []\n",
    "        for lat, lon in zip(location_lats, location_lons):\n",
    "            if data_type == 'omon':\n",
    "                distance = np.sqrt((lats - lat)**2 + (lons - lon)**2)\n",
    "                node_indices.append(np.unravel_index(np.argmin(distance), lats.shape))\n",
    "            elif data_type == 'amon':\n",
    "                lat_idx = (np.abs(lats - lat)).argmin()\n",
    "                lon_idx = (np.abs(lons - lon)).argmin()\n",
    "                node_indices.append((lat_idx, lon_idx))\n",
    "\n",
    "        for t in range(len(times)):\n",
    "            for name, (i, j) in zip(location_names, node_indices):\n",
    "                if data_type == 'omon':\n",
    "                    values = dataset.variables[var][t, :, i, j]\n",
    "                    time_series_data[name].append([times[t]] + values.tolist())\n",
    "                elif data_type == 'amon':\n",
    "                    var_value = dataset.variables[var][t, i, j]\n",
    "                    time_series_data[name].append([times[t], var_value])\n",
    "        \n",
    "        dataset.close()\n",
    "\n",
    "    for name in location_names:\n",
    "        if data_type == 'omon':\n",
    "            df = pd.DataFrame(time_series_data[name], columns=['Date'] + [f'{var}_depth_{d}' for d in range(50)])\n",
    "            df.to_csv(f'{name}_{var}_{ssp}_ts_alldepths.csv', index=False)\n",
    "            print(f'The file {name}_{var}_{ssp}_ts_alldepths.csv has been written.')\n",
    "        elif data_type == 'amon':\n",
    "            df = pd.DataFrame(time_series_data[name], columns=['Date', var])\n",
    "            df.to_csv(f'{name}_{var}_{ssp}_ts.csv', index=False)\n",
    "            print(f'The file {name}_{var}_{ssp}_ts.csv has been written.')\n",
    "\n",
    "\n",
    "if location_type == 'domain':\n",
    "    sample_dataset = nc.Dataset(filepath, mode='r')\n",
    "    if data_type == 'omon':\n",
    "        lats = sample_dataset.variables['latitude'][:]\n",
    "        lons = sample_dataset.variables['longitude'][:]\n",
    "    elif data_type == 'amon':\n",
    "        lats = sample_dataset.variables['lat'][:]\n",
    "        lons = sample_dataset.variables['lon'][:]\n",
    "    sample_dataset.close()\n",
    "\n",
    "    lat_mask = (lats >= lat_min) & (lats <= lat_max)\n",
    "    lon_mask = (lons >= lon_min) & (lons <= lon_max)\n",
    "    unique_cells = {}\n",
    "    for i in range(lats.shape[0]):\n",
    "        for j in range(lons.shape[1]):\n",
    "            if lat_mask[i, j] and lon_mask[i, j]:\n",
    "                lat_lon_key = f'lat_{lats[i, j]:.2f}_lon_{lons[i, j]:.2f}'\n",
    "                unique_cells[lat_lon_key] = (i, j)\n",
    "\n",
    "    for key in unique_cells.keys():\n",
    "        time_series_data[key] = []\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        dataset = nc.Dataset(filepath, mode='r')\n",
    "        time = dataset.variables['time'][:]\n",
    "        time_units = dataset.variables['time'].units\n",
    "        time_calendar = dataset.variables['time'].calendar\n",
    "        times = nc.num2date(time, units=time_units, calendar=time_calendar)\n",
    "\n",
    "        for t in range(len(times)):\n",
    "            for key, (i, j) in unique_cells.items():\n",
    "                if data_type == 'omon':\n",
    "                    variable_value = dataset.variables[var][t, 0, i, j]   #Here I am specifically taking for the surface depth\n",
    "                elif data_type == 'amon':\n",
    "                    variable_value = dataset.variables[var][t, i, j]\n",
    "                time_series_data[key].append([times[t], variable_value])\n",
    "        \n",
    "        dataset.close()\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    for key in time_series_data.keys():\n",
    "        df = pd.DataFrame(time_series_data[key], columns=['Date', key])\n",
    "        combined_df = df if combined_df.empty else pd.merge(combined_df, df, on='Date')\n",
    "\n",
    "    if data_type == 'omon':\n",
    "        combined_df.to_csv(f'{location_type}_{var}_{ssp}_ts_surface.csv', index=False)\n",
    "        print(f'The file {location_type}_{var}_{ssp}_ts_surface.csv has been written.')\n",
    "    elif data_type == 'amon':\n",
    "        combined_df.to_csv(f'{location_type}_{var}_{ssp}_ts.csv', index=False)\n",
    "        print(f'The file {location_type}_{var}_{ssp}_ts.csv has been written')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec5c67-3acc-4327-bae2-0239973d43e9",
   "metadata": {},
   "source": [
    "## Shapefile based data extraction from NetCDF files directly\n",
    "\n",
    "The following snippets are for netCDF files from which you want to extract data based on a Shapefile. Please ensure the type of data you are using whether it has multiple dimensions or not. The demo script for PR (atmospheric variable, Precipitation Flux with 3 dimensions time,lat,lon) is added here for demonstration purpose. This script will give a timeseries for the cells which are extracted from the shapefile. The first column contains the date/time from start to end based on the nc file. The remaining columns represent each of the unique cells based on latitude and longitude, to be written to a csv in the same format as the previous snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b40400-e993-47a9-ac95-43f47a5d68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "\n",
    "# Load the netcdf\n",
    "nc_file_path = r'H:/Semester 4/Draft Preparations/Integrating SHP with CMCCESM2 Code/pr_3hr_CMCC-ESM2_historical_r1i1p1f1_gn_200001010130-200412312230.nc'\n",
    "ds = xr.open_dataset(nc_file_path)\n",
    "nc_file_name = os.path.basename(nc_file_path).replace('.nc','')\n",
    "parts = nc_file_name.split('_')\n",
    "variable = parts[0]\n",
    "frequency = parts[1]\n",
    "scenario = parts[3]\n",
    "\n",
    "# Load the shapefile\n",
    "shapefile = r'H:/Semester 4/Draft Preparations/Integrating SHP with CMCCESM2 Code/merged_region_interest.shp'\n",
    "shp_file_name = os.path.basename(shapefile).replace('.shp','')\n",
    "sf = gpd.read_file(shapefile)\n",
    "\n",
    "# Focus on the specific variable\n",
    "pr = ds['pr']\n",
    "\n",
    "# Set the spatial dimensions\n",
    "pr = pr.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "pr.rio.write_crs('epsg:4326', inplace=True)\n",
    "\n",
    "# For shapefile:\n",
    "sf.set_crs('epsg:4326', inplace=True, allow_override=True)\n",
    "sf = sf.to_crs(pr.rio.crs)\n",
    "\n",
    "# Clip the dataset using the shapefile\n",
    "clipped_nc = pr.rio.clip(sf.geometry, sf.crs, drop=True)\n",
    "\n",
    "# Convert the clipped data to a pandas DataFrame\n",
    "clipped_df = clipped_nc.to_dataframe().reset_index()\n",
    "\n",
    "# Remove 'spatial_ref' column\n",
    "clipped_df = clipped_df.drop(columns=['spatial_ref'])\n",
    "\n",
    "# Format the lat_lon column names\n",
    "clipped_df['lat_lon'] = clipped_df.apply(lambda row: f\"lat_{row['lat']:.2f}_lon_{row['lon']:.2f}\", axis=1)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "clipped_pivot = clipped_df.pivot(index='time', columns='lat_lon', values='pr')\n",
    "\n",
    "# # Inspect the reshaped DataFrame\n",
    "# print(clipped_pivot.head(20))\n",
    "\n",
    "# Save the reshaped DataFrame to a CSV file\n",
    "output_file_path = f'H:/Semester 4/Draft Preparations/Integrating SHP with CMCCESM2 Code/{shp_file_name}_{variable}_{frequency}_{scenario}_ts.csv'\n",
    "clipped_pivot.to_csv(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163d6d8-ce39-4b1b-b61b-7542a7755324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
